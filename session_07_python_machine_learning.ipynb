{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78457e09",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-packages\" data-toc-modified-id=\"Load-packages-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Load packages</a></span></li><li><span><a href=\"#Load-dataset:-Groundwater-chemistry-South-Australia\" data-toc-modified-id=\"Load-dataset:-Groundwater-chemistry-South-Australia-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Load dataset: Groundwater chemistry South Australia</a></span></li></ul></li><li><span><a href=\"#Clustering\" data-toc-modified-id=\"Clustering-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Clustering</a></span><ul class=\"toc-item\"><li><span><a href=\"#Preprocess-the-data\" data-toc-modified-id=\"Preprocess-the-data-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Preprocess the data</a></span></li><li><span><a href=\"#Clustering-with-DBSCAN\" data-toc-modified-id=\"Clustering-with-DBSCAN-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Clustering with DBSCAN</a></span></li></ul></li><li><span><a href=\"#Classification\" data-toc-modified-id=\"Classification-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Classification</a></span><ul class=\"toc-item\"><li><span><a href=\"#Prepare-dataset\" data-toc-modified-id=\"Prepare-dataset-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Prepare dataset</a></span></li><li><span><a href=\"#Classify-with-Random-Forests\" data-toc-modified-id=\"Classify-with-Random-Forests-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Classify with Random Forests</a></span></li><li><span><a href=\"#Nitrate-class-prediction-based-on-TDS-and-major-ions\" data-toc-modified-id=\"Nitrate-class-prediction-based-on-TDS-and-major-ions-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Nitrate class prediction based on TDS and major ions</a></span></li></ul></li><li><span><a href=\"#Regression\" data-toc-modified-id=\"Regression-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#Nitrate-regression\" data-toc-modified-id=\"Nitrate-regression-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Nitrate regression</a></span></li></ul></li><li><span><a href=\"#Dimensionality-reduction\" data-toc-modified-id=\"Dimensionality-reduction-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Dimensionality reduction</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea317cd",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This notebook shows a couple of examples of machine learning approaches, applied to a hydrochemistry dataset. There are many Python packages for machine learning. Within this notebook, we can only scratch the surface of what is possible with machine learning. Rather than focusing on ML techniques, we focus on different tasks for which machine learning can help:\n",
    "\n",
    "- clustering\n",
    "- classification\n",
    "- regression\n",
    "- dimensionality reduction\n",
    "\n",
    "This notebook uses the scikit-learn package. It is a user friendly package that provides a comprehensive, well-documented introduction to machine learning, with comparisons between techniques and plenty of example code.\n",
    "## Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e84aa0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preamble\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# allow interactive figures in notebook\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4ad473",
   "metadata": {},
   "source": [
    "## Load dataset: Groundwater chemistry South Australia\n",
    "The dataset we are using is a publicly available dataset of groundwater chemistry of South Australia.\n",
    "\n",
    "[Gray, David J. and Bardwell, Nicole (2016) Hydrogeochemistry of South Australia: Data Release: Accompanying Notes. CSIRO, Australia. EP156116 34p](https://data.csiro.au/collections/collection/CIcsiro:17862v1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff778cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'CSH_SA.xlsx'\n",
    "sheet = 'Data'\n",
    "chem = pd.read_excel(fname,sheet)\n",
    "chem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b76bfb",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "Clustering is dividing the data samples into groups, based on their properties, without having predefined labels. Some approaches require to specify how many groups or clusters are in the data, most approaches do not require this input, but have a parameter that controls the number of groups.\n",
    "\n",
    "In this example we will cluster the dataset based on the Total Dissolved Solids (TDS) and Alkalinity (HCO3). Clustering algorithms can use many more variables or dimensions. We limit ourselves to two to make it easier to visualise.\n",
    "\n",
    "## Preprocess the data\n",
    "Machine learning or data analysis techniques often work better if data have a similar scale and are (close to) normally distributed. There are many approaches to preprocess and normalise or standardise data. In this case, we just do a log10 transform.\n",
    "Most algorithms can deal with missing values, but you need to check the documentation to learn how missing values are dealt with. If you have sufficient samples, it is better to drop any samples from the analysis that have missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0342db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only select TDS & HCO3, drop any rows with missing values\n",
    "TDSHCO3 = chem[['TDSc_mgL','HCO3_mgL']].dropna() \n",
    "# plot TDS v HCO3 on log log scale\n",
    "plt.figure()\n",
    "plt.loglog(TDSHCO3['TDSc_mgL'],TDSHCO3['HCO3_mgL'],'.k',alpha=0.2)\n",
    "plt.xlabel('TDS (mg/L)')\n",
    "plt.ylabel('HCO3 (mg/L)')\n",
    "# log10 transform of dataframe\n",
    "TDSHCO3_l = np.log10(TDSHCO3) #log10 transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d53017",
   "metadata": {},
   "outputs": [],
   "source": [
    "TDSHCO3_l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6314391",
   "metadata": {},
   "source": [
    "## Clustering with DBSCAN\n",
    "There are many clustering algorithms. DBSCAN is easy to use and generally results in robust clusters. The number of clusters is controlled by the eps parameter; the smaller the number, the more clusters it returns. To avoid many clusters with a single datapoint, you can specify the minimum number of samples per cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f70b6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import DBSCAN from sklearn\n",
    "from sklearn.cluster import DBSCAN\n",
    "# convert to numpy array\n",
    "X = np.array(TDSHCO3_l)\n",
    "# remove -inf values\n",
    "X = X[~np.isinf(X).any(axis=1)]\n",
    "# call the DBSCAN function\n",
    "db = DBSCAN(eps=0.1, min_samples=10).fit(X)\n",
    "# array with cluster labels, -1 is no cluster\n",
    "labels = db.labels_\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "\n",
    "print(\"Estimated number of clusters: %d\" % n_clusters_)\n",
    "print(\"Estimated number of noise points: %d\" % n_noise_)\n",
    "\n",
    "# plot TDS v HCO3, points colored according to cluster\n",
    "unique_labels = set(labels)\n",
    "core_samples_mask = np.zeros_like(labels, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "\n",
    "colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\n",
    "plt.figure()\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    if k == -1:\n",
    "        # Black used for noise.\n",
    "        col = [0, 0, 0, 1]\n",
    "\n",
    "    class_member_mask = labels == k\n",
    "\n",
    "    xy = X[class_member_mask & core_samples_mask]\n",
    "    plt.plot(\n",
    "        xy[:, 0],\n",
    "        xy[:, 1],\n",
    "        \"o\",\n",
    "        markerfacecolor=tuple(col),\n",
    "        markeredgecolor=\"k\",\n",
    "        markersize=4,\n",
    "    )\n",
    "\n",
    "    xy = X[class_member_mask & ~core_samples_mask]\n",
    "    plt.plot(\n",
    "        xy[:, 0],\n",
    "        xy[:, 1],\n",
    "        \"o\",\n",
    "        markerfacecolor=tuple(col),\n",
    "        markeredgecolor=\"k\",\n",
    "        markersize=1,\n",
    "    )\n",
    "\n",
    "plt.title(f\"Estimated number of clusters: {n_clusters_}\\nEstimated number of noise points: {n_noise_}\")\n",
    "plt.xlabel('TDS (mg/L)')\n",
    "plt.ylabel('HCO3 (mg/L)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a95b58",
   "metadata": {},
   "source": [
    "# Classification\n",
    "Clustering is similar to classification; the goal is to assign a sample to a group based on its properties. The main difference is that clustering is unsupervised (the algorithm seeks groups that are most similar within the group and most disimilar between groups), while classification is supervised. There is a training dataset with labelled samples. The algorithm tries to learn how to label the data and then predict what group new samples belong to.\n",
    "\n",
    "In this example, we create 4 groups in the dataset, based on salinity:\n",
    "- fresh (TDS < 1000 mg/L)\n",
    "- slightly saline (1000 mg/L < TDS < 3000 mg/L)\n",
    "- moderately saline (3000 mg/L < TDS < 10000 mg/L)\n",
    "- highly saline (TDS > 10.000 mg/L)\n",
    "\n",
    "We will train the algorithm to use the major ion concentrations (HCO3, Na, K, Mg, Ca, Cl, SO4, NO3) to predict the salinity class.\n",
    "\n",
    "## Prepare dataset\n",
    "The first task is to classify the samples. I've used a helper function for this. Plotting TDS v labels helps in verifying the classification is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86749e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select variables\n",
    "sal = chem[['TDSc_mgL','HCO3_mgL','Na_mgL','K_mgL','Mg_mgL','Ca_mgL','Cl_mgL','SO4_mgL','NO3N_mgL']].dropna()\n",
    "# create labels based on TDS\n",
    "def classifier(row):\n",
    "    if row[\"TDSc_mgL\"] <= 1000:\n",
    "        return 1\n",
    "    elif (row[\"TDSc_mgL\"] > 1000) and (row[\"TDSc_mgL\"] <= 3000):\n",
    "        return 2\n",
    "    elif (row[\"TDSc_mgL\"] > 3000) and (row[\"TDSc_mgL\"] <= 10000):\n",
    "        return 3\n",
    "    else:\n",
    "        return 4\n",
    "sal['class'] = sal.apply(classifier, axis=1)\n",
    "# check labels\n",
    "plt.figure()\n",
    "plt.semilogx(sal['TDSc_mgL'],sal['class'],'.k')\n",
    "plt.xlabel('TDS (mg/L)')\n",
    "plt.ylabel('Label')\n",
    "plt.yticks(np.arange(1,5),['Fresh','Slightly saline','Moderately saline', 'Highly saline'])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dfe113",
   "metadata": {},
   "source": [
    "While salinity is a function of the total concentration of dissolved elements in the sample, it is not trivial to predict the salinity class based on major ion composition. If we plot the log concentrations of the ions against the labels, you'll see that there is no single element that can be used for the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8215585c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "for i,ion in enumerate(['HCO3_mgL','Na_mgL','K_mgL','Mg_mgL','Ca_mgL','Cl_mgL','SO4_mgL','NO3N_mgL']):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.semilogx(sal[ion],sal['class'],'.k')\n",
    "    plt.title(ion)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5a14e8",
   "metadata": {},
   "source": [
    "## Classify with Random Forests\n",
    "One of the first approaches for classification were decision trees. The algorithm tries to find ways to hierarchically split the datasets based on the properties. The approach works well, but is very sensitive to outliers. Ensemble methods have proven to be much more robust. A large number of decision trees are created by bootstrapping the data (resampling with replacement). The ensemble of decision trees is called a random forest.\n",
    "\n",
    "To train a random forest classifier, we need to split the dataset into a training dataset and a validation dataset. Here I chose to use the first 4500 samples to train the classifier. The remainder of the dataset (~1000 samples) is used for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588f2faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# select the relevant variables from the dataframe and create numpy arrays\n",
    "X = np.array(sal[['HCO3_mgL','Na_mgL','K_mgL','Mg_mgL','Ca_mgL','Cl_mgL','SO4_mgL','NO3N_mgL']])\n",
    "Y = np.array(sal['class'])\n",
    "# divide into training and validation\n",
    "X_tr = X[0:4500,:]\n",
    "Y_tr = Y[0:4500]\n",
    "X_val = X[4500::,:]\n",
    "Y_val = Y[4500::]\n",
    "# create and train RandomForestClassifier (n_estimators is number of trees; more is better, but slower)\n",
    "clf = RandomForestClassifier(n_estimators=10)\n",
    "clf = clf.fit(X_tr, Y_tr)\n",
    "# test performance by predicting salinity class for validation samples\n",
    "val = clf.predict(X_val)\n",
    "# compute a summary table with the count of correctly and incorrectly classified samples\n",
    "conf = np.zeros((4,4))\n",
    "for i in np.arange(1,5):\n",
    "    for j in np.arange(1,5):\n",
    "        conf[i-1,j-1]=np.sum((Y_val==i)&(val==j))\n",
    "conff = pd.DataFrame(conf,columns=['RF fresh','RF slightly saline','RF moderately saline', 'RF highly saline'],\n",
    "                     index=['Fresh','Slightly saline','Moderately saline', 'Highly saline'])\n",
    "conff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bad4ba7",
   "metadata": {},
   "source": [
    "## Nitrate class prediction based on TDS and major ions\n",
    "Let's try to do a similar exercise, but now label the dataset based on the nitrate concentration and try to predict the class based on TDS and major ions. The classes for nitrate are:\n",
    "- No NO3 (NO3 < 1 mg/L)\n",
    "- Low NO3 (1 mg/L < NO3 < 50 mg/L)\n",
    "- High NO3 (NO3 > 50 mg/L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3a2ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create labels based on Nitrate\n",
    "def classifier(row):\n",
    "    if row[\"NO3N_mgL\"] <= 1:\n",
    "        return 1\n",
    "    elif (row[\"NO3N_mgL\"] > 1) and (row[\"NO3N_mgL\"] <= 25):\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "sal['classNO3'] = sal.apply(classifier, axis=1)\n",
    "# check labels\n",
    "plt.figure()\n",
    "plt.semilogx(sal['NO3N_mgL'],sal['classNO3'],'.k')\n",
    "plt.xlabel('NO3 (mg/L)')\n",
    "plt.ylabel('Label')\n",
    "plt.yticks(np.arange(1,4),['No NO3','Low NO3','High NO3'])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e51452",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "for i,ion in enumerate(['TDSc_mgL','HCO3_mgL','Na_mgL','K_mgL','Mg_mgL','Ca_mgL','Cl_mgL','SO4_mgL']):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.semilogx(sal[ion],sal['classNO3'],'.k')\n",
    "    plt.title(ion)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0873824",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(sal[['TDSc_mgL','HCO3_mgL','Na_mgL','K_mgL','Mg_mgL','Ca_mgL','Cl_mgL','SO4_mgL']])\n",
    "Y = np.array(sal['classNO3'])\n",
    "X_tr = X[0:4500,:]\n",
    "Y_tr = Y[0:4500]\n",
    "X_val = X[4500::,:]\n",
    "Y_val = Y[4500::]\n",
    "clf = RandomForestClassifier(n_estimators=10)\n",
    "clf = clf.fit(X_tr, Y_tr)\n",
    "val = clf.predict(X_val)\n",
    "# compute a summary table with the count of correctly and incorrectly classified samples\n",
    "conf = np.zeros((3,3))\n",
    "for i in np.arange(1,4):\n",
    "    for j in np.arange(1,4):\n",
    "        conf[i-1,j-1]=np.sum((Y_val==i)&(val==j))\n",
    "conff = pd.DataFrame(conf,columns=['RF no NO3','RF low NO3','RF high NO3'],\n",
    "                     index=['no NO3','low NO3','high NO3'])\n",
    "conff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e87b34f",
   "metadata": {},
   "source": [
    "From the table above it is clear that the classifier is not performing well. We can probably improve the classifier by using more samples, tweak the parameters or select a different algorithm. However, from a process understanding it is not surprising that predicting nitrate class from major ions does not work well - nitrate concentrations are controlled by landuse, vegetation and redox conditions. Major ions do not have information on those aspects.\n",
    "# Regression\n",
    "Instead of predicting classes of salinity, let's try to predict salinity values based on major ions. The random forest approach can also be used for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ae53c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import regressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# import function to calculate R2 score\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# prepare dataset again and divide in training and validation\n",
    "X = np.array(sal[['HCO3_mgL','Na_mgL','K_mgL','Mg_mgL','Ca_mgL','Cl_mgL','SO4_mgL','NO3N_mgL']])\n",
    "Y = np.log10(np.array(sal['TDSc_mgL']))\n",
    "X_tr = X[0:4500,:]\n",
    "Y_tr = Y[0:4500]\n",
    "X_val = X[4500::,:]\n",
    "Y_val = Y[4500::]\n",
    "\n",
    "# create and train RF regressor\n",
    "clf = RandomForestRegressor(n_estimators=50)\n",
    "clf = clf.fit(X_tr, Y_tr)\n",
    "# predict salinity for validation samples\n",
    "val = clf.predict(X_val)\n",
    "r2 = r2_score(Y_val,val)\n",
    "\n",
    "# plot result\n",
    "plt.figure()\n",
    "plt.loglog(10**Y_val,10**val,'.k')\n",
    "plt.title(f'Predict TDS from major ions \\n R2 = {r2:4.3f}')\n",
    "plt.xlabel('Measured TDS (mg/L)')\n",
    "plt.ylabel('Modelled TDS (mg/L)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce476660",
   "metadata": {},
   "source": [
    "## Nitrate regression\n",
    "Let's try the same but predicting NO3 values from major ions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf6256d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "X = np.array(sal[['TDSc_mgL','HCO3_mgL','Na_mgL','K_mgL','Mg_mgL','Ca_mgL','Cl_mgL','SO4_mgL']])\n",
    "Y = np.array(sal['NO3N_mgL'])\n",
    "X_tr = X[0:4500,:]\n",
    "Y_tr = Y[0:4500]\n",
    "X_val = X[4500::,:]\n",
    "Y_val = Y[4500::]\n",
    "\n",
    "clf = RandomForestRegressor(n_estimators=50)\n",
    "clf = clf.fit(X_tr, Y_tr)\n",
    "val = clf.predict(X_val)\n",
    "r2 = r2_score(Y_val,val)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(Y_val,val,'.k')\n",
    "plt.title(f'Predict NO3 from major ions \\n R2 = {r2:4.3f}')\n",
    "plt.xlabel('Measured NO3 (mg/L)')\n",
    "plt.ylabel('Modelled NO3 (mg/L)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9998501",
   "metadata": {},
   "source": [
    "# Dimensionality reduction\n",
    "Visualising high-dimensional data is very challenging. Dimensionality reduction can help by creating a projection of the data, where samples that are similar are plotted close to each other and samples that are very dissimilar are plotted far apart.\n",
    "In this example, we'll create a projection of the major ion data, color each data point based on its position and visualise that on a map. I find this kind of visualisation useful when dealing with data that represent mixtures of end members, where it is not often possible to define clear clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f244ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Lat','Long','TDSc_mgL', 'pH', 'HCO3_mgL', 'Na_mgL', 'K_mgL', 'Mg_mgL', 'Ca_mgL', 'Cl_mgL', 'SO4_mgL','NO3N_mgL']\n",
    "dat = chem[cols].dropna() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e32ef07",
   "metadata": {},
   "source": [
    "Rather than doing a log transform, I've preprocessed the data by taking the rank i.e. the position if you were to rank them from smallest to largest. This can be easily done wiith the `rankdata` function from [scipy.stats](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rankdata.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ae56e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import ranking function\n",
    "from scipy.stats import rankdata\n",
    "# calculate rank of data\n",
    "dr = rankdata(np.array(dat[cols[2::]]),axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0330e2d0",
   "metadata": {},
   "source": [
    "Here we'll look into dimensionality reduction and manifold learning. The goal is to find a representation of the data in 2D such that samples that are similar are plotted close to each other and samples that are very different are plotted far apart. The [manifold learning page](https://scikit-learn.org/stable/modules/manifold.html#manifold) gives an overview of methods you can use. The method we'll be using is [t-distributed Stochastic Neighbor Embedding](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html#sklearn.manifold.TSNE).\n",
    "\n",
    "We need to import the function from scikit learn, specify the parameters and then train the algorithm with our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16141d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, init='pca')\n",
    "X = tsne.fit_transform(dr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2702c052",
   "metadata": {},
   "source": [
    "The result is a 2D numpy array with an x and y coordinate for each sample. We can visualise this with `plt.scatter` and color the plot with the rank of each of the variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af84e901",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7,7))\n",
    "for i,col in enumerate(cols[2::]):\n",
    "    ax = plt.subplot(4,3,i+1, aspect=1)\n",
    "    ax.scatter(X[:,0],X[:,1],0.5,dr[:,i],cmap='viridis')\n",
    "    ax.set_title(col)\n",
    "    ax.set_axis_off()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9555e64",
   "metadata": {},
   "source": [
    "This is a spatial dataset, so we want to know show this information on a map. I've developed a 2D perceptually colormap that can be used to assign a unique color to each sample, based on the coordinates of the TSNE projection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91acc2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def percuniform_rgb(x,y):\n",
    "    '''\n",
    "    Create RGB values for x,y positions from perceptually uniform colour scheme\n",
    "    IN:\n",
    "        x: [nx1] array of x values\n",
    "        y: [nx1] array of y values\n",
    "    OUT:\n",
    "        rgb: [nx3] array of rgb values\n",
    "    '''\n",
    "    # rescale cartesian coordinates into range [-1,1]\n",
    "    # normalise based on max(range(x),range(y))\n",
    "    # multiply by 2 and subtract 1 to have data \n",
    "    # - centered on [0,0] \n",
    "    # - x and y each in range [-1,1]\n",
    "    range_x = x.max()-x.min()\n",
    "    range_y = y.max()-y.min()\n",
    "    range_m = max(range_x,range_y)\n",
    "    x_s = 2*((x-x.min())/range_m)-1\n",
    "    y_s = 2*((y-y.min())/range_m)-1\n",
    "    # load spline interpolant of colour scheme\n",
    "    rgb_interp = np.load('BivariateColourScheme.npy', allow_pickle=True, encoding='latin1').item()\n",
    "    # interpolate rgb values\n",
    "    rgb = np.zeros((len(x),3))\n",
    "    for i,col in enumerate(['R','G','B']):\n",
    "        rgb[:,i] = np.clip(rgb_interp[col].ev(x_s,y_s),0,1)\n",
    "    return(rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37aeb8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign color based on x,y coordinate in projected space\n",
    "tsnergb = percuniform_rgb(X[:,0],X[:,1])\n",
    "# create plot\n",
    "fig,ax = plt.subplots()\n",
    "ax.scatter(X[:,0],X[:,1],5,tsnergb)\n",
    "ax.set_aspect('equal')\n",
    "ax.set_title('Color based on position')\n",
    "ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861c95ce",
   "metadata": {},
   "source": [
    "We can now make a map of the samples, where each sample is colored based on its location in the TSNE plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc90d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig,ax = plt.subplots()\n",
    "# modify the default figure size\n",
    "fig.set_size_inches(7,7)\n",
    "ax.set_facecolor(\"whitesmoke\")\n",
    "ax.set_title('Color based on TSNE projection')\n",
    "# zoom in to SA data\n",
    "ax.set_xlim(chem['Long'].min(),chem['Long'].max())\n",
    "ax.set_ylim(chem['Lat'].min(),chem['Lat'].max())\n",
    "# use scatter to plot point with color based on recharge\n",
    "s = ax.scatter(dat['Long'],dat['Lat'],15,tsnergb)\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
