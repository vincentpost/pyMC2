{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Pandas to process logger data (continued)\n",
    "\n",
    "This notebook is another demonstration of how to use Pandas to read and process data from pressure transducers. \n",
    "1. Load the data\n",
    "2. Check when the baro logger was submerged\n",
    "3. Replace the faulty readings with those from a nearby logger\n",
    "4. Subtract the atmospheric pressure to obtain the water column height above the logger\n",
    "5. Determine the flow duration curve\n",
    "\n",
    "First the required libraries must be imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `read_csv` command is similar to the one we used before, except that the decimal separator in this file is a comma and not a dot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = Path(\"data\", \"Douka Longo\", \"Douka_Longo_210818202752_X0709.CSV\")\n",
    "df_wl0 = pd.read_csv(\n",
    "    fpath,\n",
    "    skiprows=63,\n",
    "    skipfooter=1,\n",
    "    sep=';',\n",
    "    decimal=',', # Decimal separator is required for this file\n",
    "    index_col=0,\n",
    "    parse_dates=True,\n",
    "    encoding=\"ISO-8859-1\",\n",
    "    engine='python',\n",
    ")\n",
    "df_wl0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For consistency with the previous exercise, let's convert the pressures from centimeters to meters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wl0[\"Pressure[mH2O]\"] = df_wl0[\"Pressure[cmH2O]\"] / 100\n",
    "df_wl0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the data were downloaded for the first time, the logger was restarted and set to record at 15 minute intervals. This means that we have to import this data and resammple it to 1 hour time interval to be consistent with the previous set of readings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = Path(\"data\", \"Douka Longo\", \"Douka_Longo_230707094904_X0709.CSV\")\n",
    "df_wl1 = pd.read_csv(\n",
    "    fpath,\n",
    "    skiprows=63,\n",
    "    skipfooter=1,\n",
    "    sep=';',\n",
    "    decimal=',', # Decimal separator is required for this file\n",
    "    index_col=0,\n",
    "    parse_dates=True,\n",
    "    encoding=\"ISO-8859-1\",\n",
    "    engine='python',\n",
    ")\n",
    "df_wl1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We resample by taking the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wl1_h = df_wl1.resample('1H').mean()\n",
    "df_wl1_h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataFrames can then be joined using the `concat` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wl_combi = pd.concat([df_wl0, df_wl1_h])\n",
    "df_wl_combi[\"Pressure[mH2O]\"].plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the figure it is not immediately obvious that there is a gap in the time series. Another way to create a single DataFrame from the two imported time series is to first create a DataFrame with a DateRange as the index and a single column with only NaN values. The `fillna` method then conveniently replaces any NaN values using either the data from `df_wl0` or `df_wl1_h`. From the plot it then becomes obvious that there is a gap in the time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_index = pd.date_range(start=df_wl0.index[0], end=df_wl1.index[-1], freq='1H')\n",
    "df_wl_combi_alt = pd.DataFrame(index=date_index, data={\"Pressure[mH2O]\": np.nan})\n",
    "df_wl_combi_alt[\"Pressure[mH2O]\"] = df_wl_combi_alt[\"Pressure[mH2O]\"].fillna(df_wl0[\"Pressure[mH2O]\"])\n",
    "df_wl_combi_alt[\"Pressure[mH2O]\"] = df_wl_combi_alt[\"Pressure[mH2O]\"].fillna(df_wl1_h[\"Pressure[mH2O]\"])\n",
    "df_wl_combi_alt[\"Pressure[mH2O]\"].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the data from the logger that periodically gets submerged and process the data in the same way as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the first file\n",
    "fpath = Path(\"data\", \"Douka Longo\", \"Douka_Longo_Baro_210818202752_BX056.CSV\")\n",
    "df_p0 = pd.read_csv(\n",
    "    fpath,\n",
    "    skiprows=51,\n",
    "    skipfooter=1,\n",
    "    sep=';',\n",
    "    decimal=',', # Decimal separator is required for this file\n",
    "    index_col=0,\n",
    "    parse_dates=True,\n",
    "    encoding=\"ISO-8859-1\",\n",
    "    engine='python',\n",
    ")\n",
    "\n",
    "# and the second\n",
    "fpath = Path(\"data\", \"Douka Longo\", \"Douka_Longo_Baro_230707104009_BX056.CSV\")\n",
    "df_p1 = pd.read_csv(\n",
    "    fpath,\n",
    "    skiprows=51,\n",
    "    skipfooter=1,\n",
    "    sep=';',\n",
    "    decimal=',', # Decimal separator is required for this file\n",
    "    index_col=0,\n",
    "    parse_dates=True,\n",
    "    encoding=\"ISO-8859-1\",\n",
    "    engine='python',\n",
    ")\n",
    "\n",
    "# Resample to one-hour intervals\n",
    "df_p1_h = df_p1.resample('1H').mean()\n",
    "\n",
    "# Create a single time series\n",
    "date_index = pd.date_range(start=df_p0.index[0], end=df_p1.index[-1], freq='1H')\n",
    "df_p_combi_do = pd.DataFrame(index=date_index, data={\"Pressure[mH2O]\": np.nan})\n",
    "df_p_combi_do[\"Pressure[mH2O]\"] = df_p_combi_do[\"Pressure[mH2O]\"].fillna(df_p0[\"Pressure[cmH2O]\"].divide(100))\n",
    "df_p_combi_do[\"Pressure[mH2O]\"] = df_p_combi_do[\"Pressure[mH2O]\"].fillna(df_p1_h[\"Pressure[mH2O]\"])\n",
    "df_p_combi_do[\"Pressure[mH2O]\"].plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are yet two more file with the data from another baro logger located nearby, which was never flooded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the first file\n",
    "fpath = Path(\"data\", \"Douka Longo\", \"Rabingha_Baro_210818202752_BX059.CSV\")\n",
    "df_p0_ra = pd.read_csv(\n",
    "    fpath,\n",
    "    skiprows=51,\n",
    "    skipfooter=1,\n",
    "    sep=';',\n",
    "    decimal=',', # Decimal separator is required for this file\n",
    "    index_col=0,\n",
    "    parse_dates=True,\n",
    "    encoding=\"ISO-8859-1\",\n",
    "    engine='python',\n",
    ")\n",
    "\n",
    "# and the second\n",
    "fpath = Path(\"data\", \"Douka Longo\", \"Rabingha_Baro_230705125953_BX059.CSV \")\n",
    "df_p1_ra = pd.read_csv(\n",
    "    fpath,\n",
    "    skiprows=51,\n",
    "    skipfooter=1,\n",
    "    sep=';',\n",
    "    decimal=',', # Decimal separator is required for this file\n",
    "    index_col=0,\n",
    "    parse_dates=True,\n",
    "    encoding=\"ISO-8859-1\",\n",
    "    engine='python',\n",
    ")\n",
    "\n",
    "# Resample to one-hour intervals\n",
    "df_p1_h_ra = df_p1_ra.resample('1H').mean()\n",
    "df_p1_h_ra\n",
    "\n",
    "# Create a single time series\n",
    "date_index = pd.date_range(start=df_p0.index[0], end=df_p1.index[-1], freq='1H')\n",
    "df_p_combi_ra = pd.DataFrame(index=date_index, data={\"Pressure[mH2O]\": np.nan})\n",
    "df_p_combi_ra[\"Pressure[mH2O]\"] = df_p_combi_ra[\"Pressure[mH2O]\"].fillna(df_p0_ra[\"Pressure[cmH2O]\"].divide(100))\n",
    "df_p_combi_ra[\"Pressure[mH2O]\"] = df_p_combi_ra[\"Pressure[mH2O]\"].fillna(df_p1_h_ra[\"Pressure[mH2O]\"])\n",
    "df_p_combi_ra[\"Pressure[mH2O]\"].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the time series in a single plot, adding an offset to account for the systematic difference in readings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "dp = 0.121\n",
    "ax.plot(df_p_combi_do[\"Pressure[mH2O]\"])\n",
    "ax.plot(df_p_combi_ra[\"Pressure[mH2O]\"] + dp); \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph above makes it clear when the logger at Douka Longo was submerged. We can try to isolate these values and plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "idx = df_p_combi_do[\"Pressure[mH2O]\"] > (df_p_combi_ra[\"Pressure[mH2O]\"] + dp + 0.04)\n",
    "ax.plot(df_p_combi_do.loc[idx, \"Pressure[mH2O]\"])\n",
    "ax.plot(df_p_combi_do.loc[~idx, \"Pressure[mH2O]\"])\n",
    "# ax.set_xlim(pd.to_datetime(\"2022-07-01\"), pd.to_datetime(\"2022-10-01\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set the pressure readings to NaN during the times when the logger was submerged. Plotting the time series and zooming in on a three-month period shows the data gaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p_combi_do[\"Pressure[mH2O]_ra\"] = df_p_combi_do[\"Pressure[mH2O]\"]\n",
    "df_p_combi_do.loc[idx, \"Pressure[mH2O]_ra\"] = np.nan\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(df_p_combi_do[\"Pressure[mH2O]_ra\"])\n",
    "ax.set_xlim(pd.to_datetime(\"2022-07-01\"), pd.to_datetime(\"2022-10-01\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like before the `fillna` method can be used to fill the gaps using the readings from `df_p_combi_ra`. The plot shows the gaps have been filled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p_combi_do[\"Pressure[mH2O]_ra\"] = df_p_combi_do[\"Pressure[mH2O]_ra\"].fillna(df_p_combi_ra[\"Pressure[mH2O]\"].add(dp))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(df_p_combi_do[\"Pressure[mH2O]_ra\"])\n",
    "ax.set_xlim(pd.to_datetime(\"2022-07-01\"), pd.to_datetime(\"2022-10-01\"));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step is to calculate the water pressures by subtracting the atmospheric pressures from the total pressures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wl_combi_alt[\"patm\"] = df_p_combi_do[\"Pressure[mH2O]_ra\"]\n",
    "df_wl_combi_alt[\"wl_corr\"] = df_wl_combi_alt[\"Pressure[mH2O]\"] - df_wl_combi_alt[\"patm\"]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(df_wl_combi_alt[\"wl_corr\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flow duration curves\n",
    "\n",
    "In principle, these water level readings could be converted to a stream discharge using a rating curve (a relationship between the discharge and the water level). Because we don't have a rating curve, we can treat the water levels as a proxy for the discharge and use the numerical values to demonstrate the principle of creating a flow duration curve. First, let's resample the water levels to daily values (and simply taking the mean)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfd = df_wl_combi_alt[\"wl_corr\"].resample('1D').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A flow duration curve displays the flow as a function of the exceedence, and the flows are plotted in the order of their size, from highest to lowest. Because `dfd` is a Pandas Series, we can use the `sort_values` method. Note that we have to get rid of some of the NaN remaining in the time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfd_sorted = dfd.sort_values(ascending=False)\n",
    "dfd_sorted = dfd_sorted.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data are sorted, the exceendence is simply inferred from the number of daily water level readings and the plot can be made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exceedence = np.arange(1.,len(dfd_sorted) + 1) / len(dfd_sorted) * 100 \n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(exceedence, dfd_sorted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geopandas_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
